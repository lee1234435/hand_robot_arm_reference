{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 10, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    'O',\n",
    "    'X'\n",
    "]\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_1_1721636009_seq10.npy'),\n",
    "    np.load('dataset/seq_2_1721636009_seq10.npy'),\n",
    "    np.load('dataset/seq_3_1721636009_seq10.npy'),\n",
    "    np.load('dataset/seq_O_1721636009_seq10.npy'),\n",
    "    np.load('dataset/seq_X_1721636009_seq10.npy')\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 10, 99)\n",
      "(193,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 17:23:39.498964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 17:23:39.616774: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-22 17:23:40.037941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lee/Desktop/xarm_ros2_moveit/install/xarm_planner/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_moveit_servo/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_gazebo/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_controller/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_api/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_sdk/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_msgs/lib:/home/lee/Desktop/xarm_ros2_moveit/install/realsense_gazebo_plugin/lib:/home/lee/Desktop/Aris_Team5_build_test/install/robot_service/lib:/home/lee/Desktop/Aris_Team5_build_test/install/interface_package/lib:/usr/local/cuda-11.8/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2024-07-22 17:23:40.038016: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lee/Desktop/xarm_ros2_moveit/install/xarm_planner/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_moveit_servo/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_gazebo/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_controller/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_api/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_sdk/lib:/home/lee/Desktop/xarm_ros2_moveit/install/xarm_msgs/lib:/home/lee/Desktop/xarm_ros2_moveit/install/realsense_gazebo_plugin/lib:/home/lee/Desktop/Aris_Team5_build_test/install/robot_service/lib:/home/lee/Desktop/Aris_Team5_build_test/install/interface_package/lib:/usr/local/cuda-11.8/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2024-07-22 17:23:40.038022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(193, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173, 10, 99) (173, 5)\n",
      "(20, 10, 99) (20, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                41984     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,229\n",
      "Trainable params: 44,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 17:23:45.831444: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/6 [====>.........................] - ETA: 4s - loss: 8.6371 - acc: 0.1875\n",
      "Epoch 1: val_acc improved from -inf to 0.65000, saving model to models/model2_1.1.h5\n",
      "6/6 [==============================] - 1s 43ms/step - loss: 5.0151 - acc: 0.3931 - val_loss: 0.8403 - val_acc: 0.6500 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.6855 - acc: 0.6875\n",
      "Epoch 2: val_acc improved from 0.65000 to 1.00000, saving model to models/model2_1.1.h5\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7184 - acc: 0.8035 - val_loss: 0.0365 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1185 - acc: 0.9375\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.7852 - acc: 0.9364 - val_loss: 0.0046 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.3246 - acc: 0.9375\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1228 - acc: 0.9711 - val_loss: 1.1698 - val_acc: 0.9500 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9081 - acc: 0.9062\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3530 - acc: 0.9827 - val_loss: 1.9359e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0399e-05 - acc: 1.0000\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.8469e-05 - acc: 1.0000 - val_loss: 1.2278e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3325e-05 - acc: 1.0000\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1948e-05 - acc: 1.0000 - val_loss: 8.5949e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1322e-06 - acc: 1.0000\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.4956e-06 - acc: 1.0000 - val_loss: 1.5812e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8391e-06 - acc: 1.0000\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0377e-05 - acc: 1.0000 - val_loss: 1.9406e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4463e-06 - acc: 1.0000\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0533e-06 - acc: 1.0000 - val_loss: 1.4626e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3883e-05 - acc: 1.0000\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.2724e-06 - acc: 1.0000 - val_loss: 9.5304e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1218e-06 - acc: 1.0000\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.8091e-06 - acc: 1.0000 - val_loss: 6.2703e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4148e-06 - acc: 1.0000\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.6453e-06 - acc: 1.0000 - val_loss: 4.2855e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4459e-06 - acc: 1.0000\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8322e-06 - acc: 1.0000 - val_loss: 3.1888e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6019e-06 - acc: 1.0000\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.3623e-06 - acc: 1.0000 - val_loss: 2.5689e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4156e-06 - acc: 1.0000\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1115e-06 - acc: 1.0000 - val_loss: 2.1875e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7858e-07 - acc: 1.0000\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9.5712e-07 - acc: 1.0000 - val_loss: 1.9014e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7800e-07 - acc: 1.0000\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.4135e-07 - acc: 1.0000 - val_loss: 1.7047e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7113e-07 - acc: 1.0000\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7.5729e-07 - acc: 1.0000 - val_loss: 1.5557e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1036e-07 - acc: 1.0000\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.9183e-07 - acc: 1.0000 - val_loss: 1.4603e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2212e-07 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.3257e-07 - acc: 1.0000 - val_loss: 1.3590e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9546e-07 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.8984e-07 - acc: 1.0000 - val_loss: 1.2696e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4762e-07 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.4781e-07 - acc: 1.0000 - val_loss: 1.2040e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3644e-07 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.2094e-07 - acc: 1.0000 - val_loss: 1.1444e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3528e-07 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.8786e-07 - acc: 1.0000 - val_loss: 1.0967e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0175e-07 - acc: 1.0000\n",
      "Epoch 26: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.6099e-07 - acc: 1.0000 - val_loss: 1.0550e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6939e-07 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.3894e-07 - acc: 1.0000 - val_loss: 1.0073e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8429e-07 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.1620e-07 - acc: 1.0000 - val_loss: 9.7751e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0920e-07 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.9553e-07 - acc: 1.0000 - val_loss: 9.5367e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9372e-07 - acc: 1.0000\n",
      "Epoch 30: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.8106e-07 - acc: 1.0000 - val_loss: 9.1791e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9430e-07 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.6383e-07 - acc: 1.0000 - val_loss: 8.8811e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5134e-07 - acc: 1.0000\n",
      "Epoch 32: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.4867e-07 - acc: 1.0000 - val_loss: 8.5234e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7195e-07 - acc: 1.0000\n",
      "Epoch 33: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3.3489e-07 - acc: 1.0000 - val_loss: 8.1658e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4587e-07 - acc: 1.0000\n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.2180e-07 - acc: 1.0000 - val_loss: 7.9274e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5076e-07 - acc: 1.0000\n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.0733e-07 - acc: 1.0000 - val_loss: 7.6890e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2096e-07 - acc: 1.0000\n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9699e-07 - acc: 1.0000 - val_loss: 7.5698e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3528e-07 - acc: 1.0000\n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.8252e-07 - acc: 1.0000 - val_loss: 7.2718e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0175e-07 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.7563e-07 - acc: 1.0000 - val_loss: 7.0929e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.6529e-07 - acc: 1.0000 - val_loss: 7.0333e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0862e-07 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.5633e-07 - acc: 1.0000 - val_loss: 6.7949e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6822e-07 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.4807e-07 - acc: 1.0000 - val_loss: 6.6161e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4214e-07 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.3842e-07 - acc: 1.0000 - val_loss: 6.4969e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6764e-07 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2946e-07 - acc: 1.0000 - val_loss: 6.2585e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7881e-07 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2602e-07 - acc: 1.0000 - val_loss: 6.2585e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1979e-07 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1844e-07 - acc: 1.0000 - val_loss: 6.0797e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0117e-07 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1292e-07 - acc: 1.0000 - val_loss: 5.7220e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7136e-07 - acc: 1.0000\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0603e-07 - acc: 1.0000 - val_loss: 5.7220e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4529e-07 - acc: 1.0000\n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0121e-07 - acc: 1.0000 - val_loss: 5.6624e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3039e-07 - acc: 1.0000\n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.9707e-07 - acc: 1.0000 - val_loss: 5.6028e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0547e-07 - acc: 1.0000\n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9087e-07 - acc: 1.0000 - val_loss: 5.4240e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2724e-07 - acc: 1.0000\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.8743e-07 - acc: 1.0000 - val_loss: 5.3644e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7195e-07 - acc: 1.0000\n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8260e-07 - acc: 1.0000 - val_loss: 5.2452e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9372e-07 - acc: 1.0000\n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7847e-07 - acc: 1.0000 - val_loss: 5.2452e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8626e-07 - acc: 1.0000\n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7778e-07 - acc: 1.0000 - val_loss: 5.2452e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0862e-07 - acc: 1.0000\n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7640e-07 - acc: 1.0000 - val_loss: 5.2452e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 56: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7158e-07 - acc: 1.0000 - val_loss: 5.2452e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3784e-07 - acc: 1.0000\n",
      "Epoch 57: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6882e-07 - acc: 1.0000 - val_loss: 5.1260e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 58: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6813e-07 - acc: 1.0000 - val_loss: 5.1260e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7509e-07 - acc: 1.0000\n",
      "Epoch 59: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6607e-07 - acc: 1.0000 - val_loss: 5.0068e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5332e-07 - acc: 1.0000\n",
      "Epoch 60: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6538e-07 - acc: 1.0000 - val_loss: 4.8876e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9744e-07 - acc: 1.0000\n",
      "Epoch 61: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6400e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7136e-07 - acc: 1.0000\n",
      "Epoch 62: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6055e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.5646e-07 - acc: 1.0000\n",
      "Epoch 63: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5918e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0803e-07 - acc: 1.0000\n",
      "Epoch 64: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5642e-07 - acc: 1.0000 - val_loss: 4.8876e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4901e-07 - acc: 1.0000\n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5573e-07 - acc: 1.0000 - val_loss: 4.8876e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5435e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8999e-07 - acc: 1.0000\n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5366e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5366e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8626e-07 - acc: 1.0000\n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5228e-07 - acc: 1.0000 - val_loss: 4.8280e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8626e-07 - acc: 1.0000\n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.4884e-07 - acc: 1.0000 - val_loss: 4.7684e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6764e-07 - acc: 1.0000\n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.4953e-07 - acc: 1.0000 - val_loss: 4.6492e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9744e-07 - acc: 1.0000\n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.4884e-07 - acc: 1.0000 - val_loss: 4.5896e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6858e-08 - acc: 1.0000\n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.4815e-07 - acc: 1.0000 - val_loss: 4.5896e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0803e-07 - acc: 1.0000\n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.4539e-07 - acc: 1.0000 - val_loss: 4.4107e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3039e-07 - acc: 1.0000\n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.4195e-07 - acc: 1.0000 - val_loss: 4.4107e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0117e-07 - acc: 1.0000\n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.3919e-07 - acc: 1.0000 - val_loss: 4.4107e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3411e-07 - acc: 1.0000\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.3713e-07 - acc: 1.0000 - val_loss: 4.3511e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4703e-08 - acc: 1.0000\n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3506e-07 - acc: 1.0000 - val_loss: 4.3511e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4901e-07 - acc: 1.0000\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.3575e-07 - acc: 1.0000 - val_loss: 4.3511e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.3575e-07 - acc: 1.0000 - val_loss: 4.3511e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8999e-07 - acc: 1.0000\n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.3437e-07 - acc: 1.0000 - val_loss: 4.3511e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2293e-07 - acc: 1.0000\n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.3161e-07 - acc: 1.0000 - val_loss: 4.2915e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9744e-07 - acc: 1.0000\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.3092e-07 - acc: 1.0000 - val_loss: 4.2319e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7136e-07 - acc: 1.0000\n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2817e-07 - acc: 1.0000 - val_loss: 4.2319e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9407e-08 - acc: 1.0000\n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2817e-07 - acc: 1.0000 - val_loss: 4.2319e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2748e-07 - acc: 1.0000 - val_loss: 4.2319e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6858e-08 - acc: 1.0000\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2541e-07 - acc: 1.0000 - val_loss: 4.1723e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2610e-07 - acc: 1.0000 - val_loss: 4.1127e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2541e-07 - acc: 1.0000 - val_loss: 4.0531e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2472e-07 - acc: 1.0000 - val_loss: 4.0531e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2293e-07 - acc: 1.0000\n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2403e-07 - acc: 1.0000 - val_loss: 4.0531e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2403e-07 - acc: 1.0000 - val_loss: 4.0531e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4529e-07 - acc: 1.0000\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2197e-07 - acc: 1.0000 - val_loss: 3.9339e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2128e-07 - acc: 1.0000 - val_loss: 3.8743e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1990e-07 - acc: 1.0000 - val_loss: 3.8743e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2666e-07 - acc: 1.0000\n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1990e-07 - acc: 1.0000 - val_loss: 3.8743e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3784e-07 - acc: 1.0000\n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1783e-07 - acc: 1.0000 - val_loss: 3.8743e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4506e-08 - acc: 1.0000\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1714e-07 - acc: 1.0000 - val_loss: 3.8743e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6764e-07 - acc: 1.0000\n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1576e-07 - acc: 1.0000 - val_loss: 3.8147e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1507e-07 - acc: 1.0000 - val_loss: 3.7551e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1370e-07 - acc: 1.0000 - val_loss: 3.7551e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2293e-07 - acc: 1.0000\n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1370e-07 - acc: 1.0000 - val_loss: 3.7551e-07 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 103/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0803e-07 - acc: 1.0000\n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1301e-07 - acc: 1.0000 - val_loss: 3.6955e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 104/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4901e-07 - acc: 1.0000\n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1301e-07 - acc: 1.0000 - val_loss: 3.6955e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 105/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3411e-07 - acc: 1.0000\n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1301e-07 - acc: 1.0000 - val_loss: 3.6955e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 106/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2293e-07 - acc: 1.0000\n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1301e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 107/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1301e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 108/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1163e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 109/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6391e-07 - acc: 1.0000\n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1094e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 110/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1025e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 111/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1025e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 112/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9605e-08 - acc: 1.0000\n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1025e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 113/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4156e-07 - acc: 1.0000\n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0956e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 114/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3039e-07 - acc: 1.0000\n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0956e-07 - acc: 1.0000 - val_loss: 3.6359e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 115/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4529e-07 - acc: 1.0000\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0887e-07 - acc: 1.0000 - val_loss: 3.5763e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 116/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9407e-08 - acc: 1.0000\n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.0750e-07 - acc: 1.0000 - val_loss: 3.5167e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 117/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6858e-08 - acc: 1.0000\n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0681e-07 - acc: 1.0000 - val_loss: 3.5167e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 118/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0681e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 119/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3784e-07 - acc: 1.0000\n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0543e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 120/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0543e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 121/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0543e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 122/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0336e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 123/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0267e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 124/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0267e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 125/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0267e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 126/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4156e-07 - acc: 1.0000\n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0198e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 127/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0198e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 128/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6764e-07 - acc: 1.0000\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0198e-07 - acc: 1.0000 - val_loss: 3.4571e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 129/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8429e-08 - acc: 1.0000\n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0129e-07 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 130/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0129e-07 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 131/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3411e-07 - acc: 1.0000\n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0129e-07 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 132/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0060e-07 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 133/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.9915e-08 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 134/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2666e-07 - acc: 1.0000\n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.9226e-08 - acc: 1.0000 - val_loss: 3.3975e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 135/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 135: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.9226e-08 - acc: 1.0000 - val_loss: 3.3379e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 136/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0058e-07 - acc: 1.0000\n",
      "Epoch 136: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.7848e-08 - acc: 1.0000 - val_loss: 3.3379e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 137/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 137: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.7848e-08 - acc: 1.0000 - val_loss: 3.3379e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 138/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9605e-08 - acc: 1.0000\n",
      "Epoch 138: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.7159e-08 - acc: 1.0000 - val_loss: 3.3379e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 139/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6858e-08 - acc: 1.0000\n",
      "Epoch 139: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.7159e-08 - acc: 1.0000 - val_loss: 3.3379e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 140/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 140: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.7159e-08 - acc: 1.0000 - val_loss: 3.2783e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 141/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1956e-08 - acc: 1.0000\n",
      "Epoch 141: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.6470e-08 - acc: 1.0000 - val_loss: 3.2783e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 142/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3039e-07 - acc: 1.0000\n",
      "Epoch 142: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.6470e-08 - acc: 1.0000 - val_loss: 3.2783e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 143/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.5646e-07 - acc: 1.0000\n",
      "Epoch 143: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.5092e-08 - acc: 1.0000 - val_loss: 3.2783e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 144/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2666e-07 - acc: 1.0000\n",
      "Epoch 144: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.4403e-08 - acc: 1.0000 - val_loss: 3.2186e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 145/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9407e-08 - acc: 1.0000\n",
      "Epoch 145: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.4403e-08 - acc: 1.0000 - val_loss: 3.2186e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 146/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6019e-07 - acc: 1.0000\n",
      "Epoch 146: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.4403e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 147/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 147: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.3714e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 148/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 148: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.3714e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 149: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.2335e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 150: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.2335e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1956e-08 - acc: 1.0000\n",
      "Epoch 151: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.1590e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0058e-07 - acc: 1.0000\n",
      "Epoch 152: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 153/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 153: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 154/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8429e-08 - acc: 1.0000\n",
      "Epoch 154: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 155/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 155: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 156/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2293e-07 - acc: 1.0000\n",
      "Epoch 156: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 9.1646e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 157/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 157: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0268e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 158/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6858e-08 - acc: 1.0000\n",
      "Epoch 158: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0268e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 159/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 159: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0268e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 160/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 160: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0268e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 161/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 161: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.0268e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 162/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4506e-08 - acc: 1.0000\n",
      "Epoch 162: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.9579e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 163/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 163: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.9579e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 164/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4156e-07 - acc: 1.0000\n",
      "Epoch 164: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.8890e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 165/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1548e-07 - acc: 1.0000\n",
      "Epoch 165: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.8890e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 166/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9605e-08 - acc: 1.0000\n",
      "Epoch 166: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.8201e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 167/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 167: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.7512e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 168/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 168: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.7512e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 169/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 169: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 8.7512e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 170/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0978e-08 - acc: 1.0000\n",
      "Epoch 170: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.6823e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 171/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2666e-07 - acc: 1.0000\n",
      "Epoch 171: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.6823e-08 - acc: 1.0000 - val_loss: 3.0398e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 172/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 172: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.6823e-08 - acc: 1.0000 - val_loss: 2.9802e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 173/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 173: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.6134e-08 - acc: 1.0000 - val_loss: 2.9802e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 174/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 174: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.5445e-08 - acc: 1.0000 - val_loss: 2.9802e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 175/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 175: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.5445e-08 - acc: 1.0000 - val_loss: 2.9802e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 176/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0058e-07 - acc: 1.0000\n",
      "Epoch 176: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.5445e-08 - acc: 1.0000 - val_loss: 2.9802e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 177/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 177: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.4756e-08 - acc: 1.0000 - val_loss: 2.9206e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 178/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 178: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.4756e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 179/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 179: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.4067e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 180/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 180: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.4067e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 181/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 181: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.4067e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 182/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5682e-08 - acc: 1.0000\n",
      "Epoch 182: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.3378e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 183/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8231e-08 - acc: 1.0000\n",
      "Epoch 183: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.3378e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 184/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1956e-08 - acc: 1.0000\n",
      "Epoch 184: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.1999e-08 - acc: 1.0000 - val_loss: 2.8610e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 185/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 185: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.1999e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 186/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 186: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.1999e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 187/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3132e-08 - acc: 1.0000\n",
      "Epoch 187: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.1999e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 188/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1176e-07 - acc: 1.0000\n",
      "Epoch 188: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.1999e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 189/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 189: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.1310e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 190/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 190: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.1310e-08 - acc: 1.0000 - val_loss: 2.8014e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 191/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8429e-08 - acc: 1.0000\n",
      "Epoch 191: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.0621e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 192/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7055e-08 - acc: 1.0000\n",
      "Epoch 192: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7.9932e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 193/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8231e-08 - acc: 1.0000\n",
      "Epoch 193: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 194/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0781e-08 - acc: 1.0000\n",
      "Epoch 194: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 195/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3330e-08 - acc: 1.0000\n",
      "Epoch 195: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 196/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 196: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 197/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8231e-08 - acc: 1.0000\n",
      "Epoch 197: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 198/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0431e-07 - acc: 1.0000\n",
      "Epoch 198: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 199: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.7418e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/200\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5879e-08 - acc: 1.0000\n",
      "Epoch 200: val_acc did not improve from 1.00000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.9243e-08 - acc: 1.0000 - val_loss: 2.6822e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model2_1.1.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[168,   1],\n",
       "        [  0,  80]],\n",
       "\n",
       "       [[163,   0],\n",
       "        [  2,  84]],\n",
       "\n",
       "       [[165,   1],\n",
       "        [  0,  83]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model2_1.1.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
